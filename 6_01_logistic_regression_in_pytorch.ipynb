{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u5633772/u5633772-DataScience-GenAI-Submissions/blob/main/6_01_logistic_regression_in_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.01 Logistic Regression in PyTorch\n",
        "This tutorial will first show you setting up a logistic regression, with a dataset we have used previously, in PyTorch. The aim is to give you a feel for how PyTorch works.\n",
        "\n",
        "We'll start with the stuff you've previously seen:"
      ],
      "metadata": {
        "id": "e7M4FXnHxhEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = datasets.load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "# Transform the test data using the *same* fitted scaler\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#Print and double check scaling has taken place\n",
        "print(\"First 5 rows of X_train_scaled:\")\n",
        "print(X_train[:5])"
      ],
      "metadata": {
        "id": "gkHWk-4IxwYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad4c680-97de-42b9-9a97-e4da5dd139e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of X_train_scaled:\n",
            "[[0.06552721 0.25769361 0.07732252 0.03436883 0.48722578 0.41750565\n",
            "  0.73336457 0.21744533 0.50400427 0.64237574 0.07818215 0.18427334\n",
            "  0.05314988 0.02029892 0.26637658 0.62943491 0.76717172 0.62928585\n",
            "  0.63623129 0.29933115 0.05964476 0.28331557 0.05597072 0.02508436\n",
            "  0.523195   0.44938009 1.         0.60137457 0.52493594 0.52950153]\n",
            " [0.65620256 0.57017247 0.67420686 0.48940187 0.55493365 0.90343127\n",
            "  0.58270853 0.74353877 0.65563267 0.50589722 0.18768785 0.08899841\n",
            "  0.17231306 0.13944393 0.08634463 0.34050831 0.09777778 0.29683652\n",
            "  0.18847288 0.1520183  0.65791974 0.57196162 0.62025316 0.46283247\n",
            "  0.52930789 0.80287939 0.54153355 0.9975945  0.49931007 0.62190573]\n",
            " [0.07257946 0.14034494 0.08023901 0.0388312  0.22190124 0.23330594\n",
            "  0.14029991 0.1083499  0.62680192 0.4142797  0.108021   0.42096888\n",
            "  0.0872167  0.03122537 0.23880749 0.2788476  0.10888889 0.29551051\n",
            "  0.63529807 0.17022512 0.04868065 0.19216418 0.05642824 0.02146189\n",
            "  0.18467704 0.15430316 0.11158147 0.174811   0.33845851 0.25313666]\n",
            " [0.14491405 0.52451809 0.14290795 0.07577448 0.3966778  0.18135744\n",
            "  0.05574039 0.08026839 0.38974907 0.28074979 0.05004527 0.25035361\n",
            "  0.03486783 0.01837512 0.18615087 0.06081954 0.02729798 0.11829892\n",
            "  0.25566464 0.05956773 0.13054601 0.61753731 0.11941437 0.05758734\n",
            "  0.53474156 0.12355454 0.08985623 0.21085911 0.363493   0.224059  ]\n",
            " [0.12140653 0.17483936 0.11829563 0.06071398 0.54861425 0.20967742\n",
            "  0.02539831 0.06411531 0.84143086 0.41364785 0.14640594 0.23886139\n",
            "  0.12038826 0.05195761 0.1971989  0.06562622 0.01935606 0.15519985\n",
            "  0.63361828 0.17475091 0.08559316 0.14472281 0.07813533 0.03597658\n",
            "  0.38259866 0.07837603 0.01731629 0.08862543 0.39266706 0.21329264]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "Scale the train data using a standard scaler/minmax scaler . Not all columns need to be standardised as some data is discrete (e.g. 'defaults on file') but selective scaling wasn't used as this can lead to inconsistent regularization penalties across different feature types.A Standard scaler was used instead, to avoid complexity and inconsistency in scaling.\n",
        "\n",
        "Always double check data has been checked:\n",
        "The data is seen to be scaled as the numberic values have changed to postive and negatives centred around 0 ( +-standard deviations from mean). Data leakage is also prevented as the scaler is only fit to X_train and the X_test is transformed with the same scaler as train."
      ],
      "metadata": {
        "id": "3pLyiSA6NcNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code takes the sklearn breast_cancer dataset, splits into train and test and then finally scales the data. Now we just need to convert this data into PyTorch's required datatype _tensors_ (see 6\\_X if you want to understand more about tensors)."
      ],
      "metadata": {
        "id": "W7-BFu0bx1x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Print an example\n",
        "print(X_train[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DLh5YBYyKln",
        "outputId": "97b239fe-3e46-4aa2-a3da-6fa596a41057"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0655, 0.2577, 0.0773, 0.0344, 0.4872, 0.4175, 0.7334, 0.2174, 0.5040,\n",
            "         0.6424, 0.0782, 0.1843, 0.0531, 0.0203, 0.2664, 0.6294, 0.7672, 0.6293,\n",
            "         0.6362, 0.2993, 0.0596, 0.2833, 0.0560, 0.0251, 0.5232, 0.4494, 1.0000,\n",
            "         0.6014, 0.5249, 0.5295],\n",
            "        [0.6562, 0.5702, 0.6742, 0.4894, 0.5549, 0.9034, 0.5827, 0.7435, 0.6556,\n",
            "         0.5059, 0.1877, 0.0890, 0.1723, 0.1394, 0.0863, 0.3405, 0.0978, 0.2968,\n",
            "         0.1885, 0.1520, 0.6579, 0.5720, 0.6203, 0.4628, 0.5293, 0.8029, 0.5415,\n",
            "         0.9976, 0.4993, 0.6219],\n",
            "        [0.0726, 0.1403, 0.0802, 0.0388, 0.2219, 0.2333, 0.1403, 0.1083, 0.6268,\n",
            "         0.4143, 0.1080, 0.4210, 0.0872, 0.0312, 0.2388, 0.2788, 0.1089, 0.2955,\n",
            "         0.6353, 0.1702, 0.0487, 0.1922, 0.0564, 0.0215, 0.1847, 0.1543, 0.1116,\n",
            "         0.1748, 0.3385, 0.2531],\n",
            "        [0.1449, 0.5245, 0.1429, 0.0758, 0.3967, 0.1814, 0.0557, 0.0803, 0.3897,\n",
            "         0.2807, 0.0500, 0.2504, 0.0349, 0.0184, 0.1862, 0.0608, 0.0273, 0.1183,\n",
            "         0.2557, 0.0596, 0.1305, 0.6175, 0.1194, 0.0576, 0.5347, 0.1236, 0.0899,\n",
            "         0.2109, 0.3635, 0.2241],\n",
            "        [0.1214, 0.1748, 0.1183, 0.0607, 0.5486, 0.2097, 0.0254, 0.0641, 0.8414,\n",
            "         0.4136, 0.1464, 0.2389, 0.1204, 0.0520, 0.1972, 0.0656, 0.0194, 0.1552,\n",
            "         0.6336, 0.1748, 0.0856, 0.1447, 0.0781, 0.0360, 0.3826, 0.0784, 0.0173,\n",
            "         0.0886, 0.3927, 0.2133]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is performing the following actions:\n",
        "\n",
        "Conversion from NumPy to PyTorch: It takes the X_train, X_test, y_train, and y_test variables, which were previously NumPy arrays, and converts them into PyTorch torch.Tensor objects.\n",
        "\n",
        "Data Type Specification: dtype=torch.float32 explicitly sets the data type for the tensors to 32-bit floating-point numbers. This is a common practice in deep learning to maintain precision while optimizing memory and computation.\n",
        "\n",
        "\n",
        "\n",
        "Adding a Dimension (.unsqueeze(1)): For y_train and y_test, the .unsqueeze(1) method is called. This adds an extra dimension of size 1 at position 1 (the second dimension). This is often done to reshape the target labels y from a 1D array (e.g., [batch_size]) to a 2D column vector (e.g., [batch_size, 1]), which is a common requirement for PyTorch's loss functions, especially when dealing with binary classification outputs like in this logistic regression model.\n",
        "\n",
        "Why?\n",
        "\n",
        "Pytorch models will naturally produce an output tensor with a shape of [batch_size, 1] when making predictions for a batch of inputs. For example, if you pass 32 samples, the model might output (32, 1). To perform element-wise comparisons and calculations (like the BCELoss does), the target tensor (y_train or y_test) needs to have a matching shape. If y were (32,) (1D), it wouldn't directly align with the (32, 1) output of the model, leading to shape mismatch errors during loss calculation.\n",
        "\n"
      ],
      "metadata": {
        "id": "NTSbStxYPvwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have sorted out the data, we can specify our model. Note this is much more customisable in PyTorch, but that also means the code is more complicated:"
      ],
      "metadata": {
        "id": "tYPOH2T7ySli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))"
      ],
      "metadata": {
        "id": "SNRzxQRvyRuv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a custom class we create called `LogisticRegression`, which inherits from PyTorch's `nn.Module` (i.e. it uses PyTorch's `nn.Module` as a template). This is the standard way to define models in PyTorch. The class encapsulates the structure and behavior of our logistic regression model.\n",
        "<br><br><br>\n",
        "__Step-by-step explanation:__\n",
        "\n",
        "1. `class LogisticRegression(nn.Module)`: This line defines the class and indicates that it's a subclass of `nn.Module`.\n",
        "2. `def __init__(self, input_dim)`: This is the constructor of the class (the thing that builds objects of this class). It takes the input_dim as an argument, which represents the number of features in the input data.\n",
        "3. `super(LogisticRegression, self).__init__()`: This line initialises the parent class (`nn.Module`), ensuring that all necessary setup is done. I.e. if we create an object of this class, then this step builds the actual object according to our template we are defining.\n",
        "4. `self.linear = nn.Linear(input_dim, 1)`: This line creates a linear layer (i.e. linear regression), which is the core component of our logistic regression model. It takes `input_dim` number of features as input and produces a single output (for binary classification).\n",
        "5. `def forward(self, x)`: This method defines the forward pass of the model, which is how input data is transformed into predictions.\n",
        "6. `return torch.sigmoid(self.linear(x))`: This line performs the following steps (steps 7 and 8):\n",
        "7. `self.linear(x)`: Applies the linear layer to the input x.\n",
        "8. `torch.sigmoid(...)`: Applies the sigmoid function to the output of the linear layer, producing a probability between 0 and 1. This probability represents the model's prediction for the given input.\n",
        "<br><br>\n",
        "Now we have defined the class (our model specification essentially), we can create an object of this class. As per the `def __init__(self, input_dim)` command above (step 2), when we build an object we need to specify the `input_dim` (number of features in the input)."
      ],
      "metadata": {
        "id": "z5HWdHFPyls9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "input_dim = X_train.shape[1]\n",
        "model = LogisticRegression(input_dim)"
      ],
      "metadata": {
        "id": "NxmLrfbF1d8w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have built an object of the class template called _model_ much as we did with sklearn (the only difference is that sklearn had the class template already built for us so all we had to do was import it). We also need to pass a couple of hyperparameters - the loss function (BCE is short for [binary cross entropy](https://docs.pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)) and the optimiser we want to use (SGD is short for [stochastic gradient descent](https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html)). If interested in how these work in PyTorch (you don't have to be), follow the links for a technical explanation."
      ],
      "metadata": {
        "id": "SaQsYW-P1j7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "VrJ2Vczy1jNd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With everything setup, we can begin training! Again this is a more manual process in PyTorch, but is still fundamentally the same:"
      ],
      "metadata": {
        "id": "clyJC6zz2W63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 1000 # specify 1000 epochs (full passes through the data)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train() # put the model object in train mode\n",
        "    optimizer.zero_grad() # reset the gradient (next week)\n",
        "    outputs = model(X_train) # pass the X_train data\n",
        "\n",
        "    # calculate loss as the comparison between predictions (y_hat) and\n",
        "    # real values (y) according to our loss function (criterion)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward() # backpropogate loss (next week)\n",
        "    optimizer.step() # update the parameters based on this round of training\n",
        "\n",
        "    # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjX1tYB72eRZ",
        "outputId": "a5e58ce2-ca3a-4381-ed91-19913c38aaeb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 0.714\n",
            "Epoch [20/1000], Loss: 0.7109\n",
            "Epoch [30/1000], Loss: 0.708\n",
            "Epoch [40/1000], Loss: 0.705\n",
            "Epoch [50/1000], Loss: 0.7021\n",
            "Epoch [60/1000], Loss: 0.6992\n",
            "Epoch [70/1000], Loss: 0.6963\n",
            "Epoch [80/1000], Loss: 0.6935\n",
            "Epoch [90/1000], Loss: 0.6906\n",
            "Epoch [100/1000], Loss: 0.6878\n",
            "Epoch [110/1000], Loss: 0.685\n",
            "Epoch [120/1000], Loss: 0.6823\n",
            "Epoch [130/1000], Loss: 0.6796\n",
            "Epoch [140/1000], Loss: 0.6768\n",
            "Epoch [150/1000], Loss: 0.6742\n",
            "Epoch [160/1000], Loss: 0.6715\n",
            "Epoch [170/1000], Loss: 0.6689\n",
            "Epoch [180/1000], Loss: 0.6662\n",
            "Epoch [190/1000], Loss: 0.6637\n",
            "Epoch [200/1000], Loss: 0.6611\n",
            "Epoch [210/1000], Loss: 0.6585\n",
            "Epoch [220/1000], Loss: 0.656\n",
            "Epoch [230/1000], Loss: 0.6535\n",
            "Epoch [240/1000], Loss: 0.651\n",
            "Epoch [250/1000], Loss: 0.6486\n",
            "Epoch [260/1000], Loss: 0.6461\n",
            "Epoch [270/1000], Loss: 0.6437\n",
            "Epoch [280/1000], Loss: 0.6413\n",
            "Epoch [290/1000], Loss: 0.6389\n",
            "Epoch [300/1000], Loss: 0.6366\n",
            "Epoch [310/1000], Loss: 0.6342\n",
            "Epoch [320/1000], Loss: 0.6319\n",
            "Epoch [330/1000], Loss: 0.6296\n",
            "Epoch [340/1000], Loss: 0.6273\n",
            "Epoch [350/1000], Loss: 0.6251\n",
            "Epoch [360/1000], Loss: 0.6228\n",
            "Epoch [370/1000], Loss: 0.6206\n",
            "Epoch [380/1000], Loss: 0.6184\n",
            "Epoch [390/1000], Loss: 0.6162\n",
            "Epoch [400/1000], Loss: 0.614\n",
            "Epoch [410/1000], Loss: 0.6119\n",
            "Epoch [420/1000], Loss: 0.6098\n",
            "Epoch [430/1000], Loss: 0.6076\n",
            "Epoch [440/1000], Loss: 0.6055\n",
            "Epoch [450/1000], Loss: 0.6035\n",
            "Epoch [460/1000], Loss: 0.6014\n",
            "Epoch [470/1000], Loss: 0.5994\n",
            "Epoch [480/1000], Loss: 0.5973\n",
            "Epoch [490/1000], Loss: 0.5953\n",
            "Epoch [500/1000], Loss: 0.5933\n",
            "Epoch [510/1000], Loss: 0.5913\n",
            "Epoch [520/1000], Loss: 0.5894\n",
            "Epoch [530/1000], Loss: 0.5874\n",
            "Epoch [540/1000], Loss: 0.5855\n",
            "Epoch [550/1000], Loss: 0.5836\n",
            "Epoch [560/1000], Loss: 0.5817\n",
            "Epoch [570/1000], Loss: 0.5798\n",
            "Epoch [580/1000], Loss: 0.5779\n",
            "Epoch [590/1000], Loss: 0.5761\n",
            "Epoch [600/1000], Loss: 0.5743\n",
            "Epoch [610/1000], Loss: 0.5724\n",
            "Epoch [620/1000], Loss: 0.5706\n",
            "Epoch [630/1000], Loss: 0.5688\n",
            "Epoch [640/1000], Loss: 0.567\n",
            "Epoch [650/1000], Loss: 0.5653\n",
            "Epoch [660/1000], Loss: 0.5635\n",
            "Epoch [670/1000], Loss: 0.5618\n",
            "Epoch [680/1000], Loss: 0.5601\n",
            "Epoch [690/1000], Loss: 0.5584\n",
            "Epoch [700/1000], Loss: 0.5567\n",
            "Epoch [710/1000], Loss: 0.555\n",
            "Epoch [720/1000], Loss: 0.5533\n",
            "Epoch [730/1000], Loss: 0.5516\n",
            "Epoch [740/1000], Loss: 0.55\n",
            "Epoch [750/1000], Loss: 0.5484\n",
            "Epoch [760/1000], Loss: 0.5467\n",
            "Epoch [770/1000], Loss: 0.5451\n",
            "Epoch [780/1000], Loss: 0.5435\n",
            "Epoch [790/1000], Loss: 0.542\n",
            "Epoch [800/1000], Loss: 0.5404\n",
            "Epoch [810/1000], Loss: 0.5388\n",
            "Epoch [820/1000], Loss: 0.5373\n",
            "Epoch [830/1000], Loss: 0.5357\n",
            "Epoch [840/1000], Loss: 0.5342\n",
            "Epoch [850/1000], Loss: 0.5327\n",
            "Epoch [860/1000], Loss: 0.5312\n",
            "Epoch [870/1000], Loss: 0.5297\n",
            "Epoch [880/1000], Loss: 0.5282\n",
            "Epoch [890/1000], Loss: 0.5268\n",
            "Epoch [900/1000], Loss: 0.5253\n",
            "Epoch [910/1000], Loss: 0.5239\n",
            "Epoch [920/1000], Loss: 0.5224\n",
            "Epoch [930/1000], Loss: 0.521\n",
            "Epoch [940/1000], Loss: 0.5196\n",
            "Epoch [950/1000], Loss: 0.5182\n",
            "Epoch [960/1000], Loss: 0.5168\n",
            "Epoch [970/1000], Loss: 0.5154\n",
            "Epoch [980/1000], Loss: 0.514\n",
            "Epoch [990/1000], Loss: 0.5127\n",
            "Epoch [1000/1000], Loss: 0.5113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that loss decreasing each time we print, which is what we would expect! But how does the module perform on test data?"
      ],
      "metadata": {
        "id": "j9LfcOsF4hku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd97eUwdxJw5",
        "outputId": "e0554354-e69c-4c63-d557-1018ebc541fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8947\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "model.eval() # switch to testing mode\n",
        "with torch.no_grad(): # turn off the gradient (stop training)\n",
        "    y_pred = model(X_test) # pass the X_test data\n",
        "\n",
        "    # if the output < 0.5 then class 0 and else class 1\n",
        "    y_pred = (y_pred >= 0.5).float()\n",
        "    accuracy = accuracy_score(y_test.numpy(), y_pred.numpy())\n",
        "    print(f'Accuracy: {round(accuracy, 4)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy looks good! How about a confusion matrix?"
      ],
      "metadata": {
        "id": "kkAA3fUr5Xdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay as CM\n",
        "import numpy as np\n",
        "\n",
        "# Convert tensors to NumPy arrays\n",
        "y_test_np = y_test.numpy()\n",
        "y_pred_np = y_pred.numpy()\n",
        "\n",
        "CM.from_predictions(y_test_np, y_pred_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "wV5nZppC5Wvv",
        "outputId": "714563fc-df62-4fb7-e213-f5964d07aab9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fc5daa30500>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the same code as earlier in the module and we can see it gives us good results! (Although you should note results might vary different times you run it because of how we use random numbers in the process - don't be scared of randomness!!).\n",
        "\n",
        "Overall we should be able to see this is a very similar process. However, unlike in sklearn, this is a much more manual process in places. This means the code is a little more hard work, but gives us much more flexibility to define the model how we want. For logistic regression we are just following the standard template of the algorithm. However, as we move on into deep learning we will see this flexibility is basically the job ... the key hyperparameters are how we define these different layers and transformations."
      ],
      "metadata": {
        "id": "02t5nq306OyT"
      }
    }
  ]
}